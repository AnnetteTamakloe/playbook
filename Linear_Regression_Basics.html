

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Linear Regression for Machine Learning: The Basics &#8212; Data_Science_Playbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Linear_Regression_Basics';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Introduction to ML: Boston Housing Data" href="Machine_Learning_BHDataset.html" />
    <link rel="prev" title="Python Basics" href="Basic_intro_Python.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/data_diaries.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/data_diaries.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    <no title>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="CommandLine.html">The Command Line</a></li>
<li class="toctree-l1"><a class="reference internal" href="IntroToGit.html">Git and Github</a></li>
<li class="toctree-l1"><a class="reference internal" href="Your_Coding_Environment.html">Your Coding Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="Basic_intro_Python.html">Python Basics</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Regression for Machine Learning: The Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="Machine_Learning_BHDataset.html">Introduction to ML: Boston Housing Data</a></li>


<li class="toctree-l1"><a class="reference internal" href="Ensemble_Learning_BH.html">Ensemble Learning: Bagging, Boosting, and Stacking</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FLinear_Regression_Basics.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Linear_Regression_Basics.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression for Machine Learning: The Basics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-exactly-is-linear-regression">What exactly is Linear Regression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-of-linear-regression">Structure of Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-determine-our-line-of-best-fit">How do we determine our line of best fit?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression-for-machine-learning-the-basics">
<h1>Linear Regression for Machine Learning: The Basics<a class="headerlink" href="#linear-regression-for-machine-learning-the-basics" title="Permalink to this heading">#</a></h1>
<p>There’s one frequent question I hear often when people inquire about the Data Science field. Do I need to know math? And to many people’s chagrin, the answer is yes, especially for Machine Learning. The good thing to know is that you DONT need to be a math wizz to understand the fundemental math concepts for the field. The beauty of applications like Python and R is that they have many tools that do much of the heavy lifting when it comes to implementing mathematical theories on your dataset, but here needs to be a willingness to understand the science behind what you create so you know how to treat your data.
Statistics in particular is a necessary curriculum you’ll need for Machine Learning and the perfect place to start is with Linear Regression. This guide will walk through the basics of using Linear Regression for Machine Learning and will hopefully act as a reference as well.</p>
<section id="what-exactly-is-linear-regression">
<h2>What exactly is Linear Regression?<a class="headerlink" href="#what-exactly-is-linear-regression" title="Permalink to this heading">#</a></h2>
<p>Linear Regression is a type of statistical analysis that is used to find a relationship between one more more features(independent variable) and a target variable (dependent variable) It can be used to forecast sales, prices, and can even used to gain insights about customer behavior. You can bucket most linear regression analyses into two types:</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p><strong>Univariate Linear Regression:</strong> Only one feature is used in your model</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p><strong>Multivariate Linear Regression:</strong> More than one feature is used in your model</p></li>
</ol>
</li>
</ul>
<section id="structure-of-linear-regression">
<h3>Structure of Linear Regression<a class="headerlink" href="#structure-of-linear-regression" title="Permalink to this heading">#</a></h3>
<p>The linear regression model can be summarized by the following equation:</p>
<p><img alt="linear_equation.png" src="_images/linear_equation.png" /></p>
<p>or</p>
<p><img alt="h(x).webp" src="attachment:e852bddd-ebc6-4b0d-a142-c40854db0fec.webp" /></p>
<ul class="simple">
<li><p>Y is the predicted value.</p></li>
<li><p>This equation is also known as the hypothesis function h(x).</p></li>
<li><p>θ1…….,θn are our model parameters or the coefficients of the features of our model. They are the foundation of our predicted values</p></li>
<li><p>θ-zero is the bias term and is also the value of the equation if all of our other features have a coefficient of 0. Its also known as the intercept</p></li>
</ul>
<p>This would also be our theoretical equation for our <em>line of best fit</em> or our <em>regression line</em>. If we plotted our feautures or independent variables individually against our target variable we would get a scatter plot. Lets create one to get a better understanding of what I mean. We will create some data for a dependent variable y and one independent variable x.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># generate random data-set</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># x is our independent variable or feature</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># y is our dependent variable or target variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># lets take a look at the relationship by using a scatter plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fe3fdb50643f9c0ec964978bb121fda10cb39e85869e77425e931830b63dceef.png" src="_images/fe3fdb50643f9c0ec964978bb121fda10cb39e85869e77425e931830b63dceef.png" />
</div>
</div>
<p>Our line of best fit’s job <em>(which represents our equation)</em> is to try and get a sense of the best or <em>optimal</em> values that would help us hit most of our observations on the scatterplot. If our line of best fit does a good job, we should be able to take it and use it to predict our Y variables fairly accurately for similar datasets that have different observations</p>
</section>
<section id="how-do-we-determine-our-line-of-best-fit">
<h3>How do we determine our line of best fit?<a class="headerlink" href="#how-do-we-determine-our-line-of-best-fit" title="Permalink to this heading">#</a></h3>
<p>As mentioned before, our line of best fits job is to hit as many points as it can on our scatter plot. If it does its job well, the distance between our line and each of our data points would be very small. The distance between each of our predicted values(represented by our line of best fit) and the actual data observations is known as an error term or a residual. Lets take a look at a visual representation of this.</p>
<p><img alt="residuals.webp" src="attachment:264bfb77-90a4-4c53-baea-1aebc0940255.webp" /></p>
<p>The sum of all of our error terms is measured by something called the <em>cost function</em> which is defined as the sum of the squares of all of our residuals. This is how our cost function looks like for a specific feature or parameter</p>
<p><img alt="cost_function.webp" src="attachment:0e572f2b-c86c-4f22-a42a-3d792f403f3e.webp" /></p>
<p><em>h(x)</em> represents the hypothesis function we mentioned before, and it is essentially the value we got for Y with our equation - our predicted value. The predicted value is subtracted from the actual value in the data to get our error for one observation. We then do this for each and every observation, add them all up and square them. The huge E seen in the equation represents the summing up all of the error terms for each of our observations. You may ask why we also square our error term. This is done so we can get the absolute values for all our observations. That way any negative values are transformed into positives before we add them up. We also do this to penalize data points which are further away from the regression line much more than the points which lie close to the line. The <em>m</em> in our equation represents the number of training sets we have</p>
<p><img alt="h(x).webp" src="attachment:98fd2259-2be3-4237-8c5e-233810d2bf13.webp" /></p>
<p>Our objective in all of this is find the best or <em>optimal</em> parameters (represented by θ) for each of our features (represented by x) to minimize the cost function or the loss function. We could do this using the Normal Equation or with something called <a class="reference external" href="https://builtin.com/data-science/gradient-descent"><strong>Gradient Descent</strong></a>.</p>
</section>
<section id="gradient-descent">
<h3>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this heading">#</a></h3>
<p>Gradient descent is an algorithm used in many machine learning algorithms. What it does is that it tweaks and changes our parameters of our model to minimize the cost function discussed before. These are the steps of gradient descent:</p>
<ol class="arabic simple">
<li><p>First you plug in some random values for your model parameters to start off with. This is known as random initialization, because we need to start off somewhere</p></li>
</ol>
<ol class="arabic simple" start="2">
<li><p>Now you have to measure how the cost function changes with each change in the parameter to see if its increasing, decreasing, ect. This can be measured by computing something called the partial derivative of the cost function which essentially captures how our parameters are changing. Our cost function would have to be convex and would have to have a global mimima, representing 0 meaning that is our optimal parameter, the lowest point on our curve. What this is essentially doing is capturing the slope as our parameter is travelling down to the global mimima. We give the algorithm a certain number of iterations to indicate how many attempts we want it to make to get to our global or local minima. This could be any number from 1 to 100000, but we need a number large enough to get it to our optimal parameter.</p></li>
</ol>
<p><img alt="derivativecostfunction.webp" src="attachment:5a8ead35-474e-4ec8-a619-4d9756845f7f.webp" /></p>
<ol class="arabic simple" start="3">
<li><p>Now after getting the derivative, you now have to update the parameters and change them simultaneously to see how it impacts our cost function. Our derivative is multiplied by something called the <em>learning rate</em> which represented by a. This learning rate will determine how fast or how slow our parameters move towards our desired local optima. We never want to have a learning rate that is too big or too small. If our learning rate is small it will take forever for us to get to our optimal parameter. But even worse, if it is too big we could completely overshoot and whizz past our desired parameters. Note that if we have more than 1 parameter, we would have to plug in the changes simultaneously to see how each change for every coefficient on our feature is impacting the cost function.</p></li>
</ol>
<p><img alt="learning_rate.webp" src="attachment:499ed091-ebf6-43d3-a901-ef01d8b0a1b6.webp" /></p>
<p><img alt="learningrate2.webp" src="attachment:afc3e393-b884-4035-9751-3f36fc48c9d5.webp" /></p>
<p>I like to think of Gradient Descent from the perspective of a hiker embarking on a journey down a valley to find a natural spring to quench their thirst. They start of at ground 0, unsure of what lies in wait for them, but step by step, put one foot in front of the other to traverse the valley on a quest to save themselves from dying of dehydration. They can’t run or walk too fast, else they might end up rolling down the hill to quickly and might shoot right past the spring, but they also cant walk too slow or they’ll be dead before they get there. They have to take slow measured steps and try their best to track their journey so they can find their way back once they find what they’re looking for.</p>
</section>
<section id="hyperparameters">
<h3>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this heading">#</a></h3>
<p>Often with machine learning algorithms, you’ll often hear the word hyperparameters. In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. With the gradient descent algorithm, our hyperparameters are our learning rate (a) and the number of iterations we need for our parameter to get to the local or global minima. We can randomly choose the values of these hyperparameteres but as I’m sure you’ve noticed, there’s always an optimization framework for anything with Machine Learning. To choose optimal hyperparameters we can employ an algorithm called <a class="reference external" href="https://www.mygreatlearning.com/blog/gridsearchcv/#:~:text=Grid%20search%20is%20a%20method,learning%20rate%20of%20the%20optimiser">GridSearch</a>. We won’t be implementing it in this example since that’s for more advanced Machine Learning models, but it will be further discussed in later blogs.</p>
<p>Lets implement the gradient descent algorithm from scratch and see how it’s able to decrease our cost function overtime</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Step 1: We have to Initialize our parameters and hyperparameters</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Theta represents our coefficients [theta0, theta1]</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="c1"># This represents our learning rate</span>
<span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">1000</span> <span class="c1"># This reprents how many times we are going to go down the curve</span>

<span class="c1"># Step 2: Define the cost function</span>
<span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">error</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cost</span>

<span class="c1"># Step 3: Calculate the gradients</span>
<span class="k">def</span> <span class="nf">compute_gradients</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">-</span> <span class="n">y</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gradients</span>

<span class="c1"># Step 4: Update the parameters</span>
<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span>
    <span class="k">return</span> <span class="n">theta</span>

<span class="c1"># Step 5: Perform gradient descent</span>
<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">compute_gradients</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
        <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">costs</span>

<span class="c1"># Add a column of ones to x for the intercept term</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">x</span><span class="p">]</span>

<span class="c1"># Perform gradient descent</span>
<span class="n">theta</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">)</span>

<span class="c1"># Print the learned parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Theta:&quot;</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

<span class="c1"># Plot the data points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>

<span class="c1"># Plot the best fit line</span>
<span class="n">x_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)])</span>
<span class="n">y_line</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_line</span><span class="p">,</span> <span class="n">y_line</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Best Fit Line&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Theta: [[2.89623533]
 [2.26925024]]
</pre></div>
</div>
<img alt="_images/9575f48528c6a4cbd0f82b123d7e78f81acdda82061e7f6edb2f47745c7f609e.png" src="_images/9575f48528c6a4cbd0f82b123d7e78f81acdda82061e7f6edb2f47745c7f609e.png" />
</div>
</div>
<p>Now that we’ve gotten our best fit line, which looks pretty good, lets now visualize how our cost function changed across each iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">),</span> <span class="n">costs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cost&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/292339a4a36608cb664257f5924d5758f813aa78bbe4a65ae8521008db389e7f.png" src="_images/292339a4a36608cb664257f5924d5758f813aa78bbe4a65ae8521008db389e7f.png" />
</div>
</div>
<p>We can see that our cost function dramatically dipped almost to 0 as we got closer to the 200th iteration, showing the how powerful the gradient descent algorithm can be for finding optimal parameters. Now lets get a more objective sense of how our moel is performing using Scikit Learns Linear Regression Model framework.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>

<span class="c1"># Model initialization</span>
<span class="n">regression_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="c1"># Fit the data(train the model)</span>
<span class="n">regression_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># Predict</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># model evaluation</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>

<span class="c1"># printing values</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Slope:&#39;</span> <span class="p">,</span><span class="n">regression_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Intercept:&#39;</span><span class="p">,</span> <span class="n">regression_model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Root mean squared error: &#39;</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R2 score: &#39;</span><span class="p">,</span> <span class="n">r2</span><span class="p">)</span>

<span class="c1"># plotting values</span>

<span class="c1"># data points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="c1"># predicted values</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Slope: [[2.93655106]]
Intercept: [2.55808002]
Root mean squared error:  0.07623324582875007
R2 score:  0.9038655568672764
</pre></div>
</div>
<img alt="_images/ea44a3f92fcfea4dd3c45364133a477bf07942397032235be8a71b9664a01fde.png" src="_images/ea44a3f92fcfea4dd3c45364133a477bf07942397032235be8a71b9664a01fde.png" />
</div>
</div>
<p>We clearly have some very good metrics here with an R2 of 90% meaning our model explains about 90% of the variation in our data and we also have a RMSE of 0.07, which is quite low and is also a good sign our model is perfomrting well.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="Basic_intro_Python.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Python Basics</p>
      </div>
    </a>
    <a class="right-next"
       href="Machine_Learning_BHDataset.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Introduction to ML: Boston Housing Data</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-exactly-is-linear-regression">What exactly is Linear Regression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-of-linear-regression">Structure of Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-determine-our-line-of-best-fit">How do we determine our line of best fit?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Annette Tamakloe
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>