{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fafad8-8c1c-478f-b354-6c685318e474",
   "metadata": {},
   "source": [
    "# Ensemble Learning: Bagging, Boosting, and Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1cbad4-0df6-4e4d-b352-edb255aca28c",
   "metadata": {},
   "source": [
    "Once you have a nice baseline for your predictive model, it's hard to think of other ways to squeeze more juice out of it. This is where [Ensemble Learning](https://builtin.com/machine-learning/ensemble-model) comes in. **Ensemble Learning** is a machine learning approach that seeks better peformance by combining predictions from *multiple* models. When we have a baseline model, that model may not perform well due to high variance or high bias as we talked about in the last blog, but when we bring several models togehter, they can create a very strong learner. Ensemble methods are also fairly easy to use. The scikit-learn library makes it easy to implement them and there usually is little to no data preprocessing since many of the ensemble algorithms include processes that handle missing data. The three main types of ensemble learning are:\n",
    "\n",
    "- [Bagging](https://www.ibm.com/topics/bagging#:~:text=the%20next%20step-,What%20is%20bagging%3F,be%20chosen%20more%20than%20once.)\n",
    "- [Boosting](https://www.ibm.com/topics/boosting) \n",
    "- [Stacking](https://developer.ibm.com/articles/stack-machine-learning-models-get-better-results/)\n",
    "\n",
    "\n",
    "\n",
    "Lets discuss and implement each one on the boston housing dataset to understand how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7adcad36-7050-461b-8cc0-b00137f06683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX   \n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0  \\\n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary packages for reading in and analyzing our dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Impute the column names \n",
    "columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE',\n",
    "           'DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']\n",
    "#Read in our dataset and define our column names and delimiter so our dataframe looks nice)\n",
    "bh = pd.read_csv('housing.data', delim_whitespace = True, header = None, names = columns)\n",
    "bh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e6c88-9c36-4c4a-8802-4ca73223cd7f",
   "metadata": {},
   "source": [
    "Let's define our features and our target variable. We'll use all our features in the dataset so we will extract the MEDV and define our features as X, and then define MEDV as y, the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbd1889-9369-4e03-b04f-1a4aec587703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (354, 13)\n",
      "Shape of X_test:  (152, 13)\n",
      "Shape of y_train:  (354,)\n",
      "Shape of y_test (152,)\n"
     ]
    }
   ],
   "source": [
    "X = bh.drop('MEDV', axis =1)\n",
    "y = bh['MEDV']\n",
    "# Lets split our training set and our testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 25)\n",
    "#Now take a look at the distributions \n",
    "print(\"Shape of X_train: \",X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b2f2d-31d3-488d-820b-c47177fa02c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a979bd4-c2f3-4715-a751-c07abed2213d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Bagging is a type of ensemble modelling where several baseline models or 'weak' models are trained at the same time and are then combined to create a better model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9f9d5-bc9e-4451-9a72-86e5338a7a80",
   "metadata": {},
   "source": [
    "![bagging_visual.png](bagging_visual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89a132-6202-49e3-bb7e-87cbf20bbae4",
   "metadata": {},
   "source": [
    "### 1. Parallel Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f499b3-9c06-4955-882a-789b8ee25427",
   "metadata": {},
   "source": [
    "First bagging uses a resampling technique known as bootstrapping to randomly pick datapoints in the training data set and puts them into several different subsets. After the subsets have been created, it then trains each of them at the same time with baseline models. This is known as **Parallel Training**. Afterwards, the average of all of the outputs are taken to create a more accurate estimation model. Bagging is usually used to enhance baseline models that have high variance and low bias which tend to be models that have overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453bbca-1e70-4f2b-abaf-a3d7a5fdaa8a",
   "metadata": {},
   "source": [
    "![bootstrap_process_bagging.png](bootstrap_process_bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933cbc1-fa22-4e29-97f6-da119176477e",
   "metadata": {},
   "source": [
    "### 2. Benefits of Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ccac68-a994-4c69-b49b-f3d0eddf5fd9",
   "metadata": {},
   "source": [
    "**Benefits of bagging include**:\n",
    "- Reduction in variance and subsequently a reduction in overfitting\n",
    "- It works well with both categorical and continuous values\n",
    "- It will most likely yield a better overall model for your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86b3ce-ebb3-4165-b6ac-249cc373a48f",
   "metadata": {},
   "source": [
    "A very popular and extremely useful bagging algorithm is **Random Forest**. We'll use the Random Forest Regression algorithm  on our dataset to understand how bagging works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f748a0f-6318-48cb-bc86-bea8a4ce47c0",
   "metadata": {},
   "source": [
    "### 3. Bagging with the Random Forest Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5b733-b8a6-405a-bee1-2cdb067353c8",
   "metadata": {},
   "source": [
    "The Random Forest Algorithm is a bagging algorithm that combines the functionalities of the [Decision Trees](https://www.cambridgespark.com/info/getting-started-with-regression-and-decision-trees) aglorithm and Bagging. For more on Decision Trees, click [here](https://www.cambridgespark.com/info/from-simple-regression-to-multiple-regression-with-decision-trees#:~:text=Decision%20trees%20can%20be%20used,the%20error%20and%20avoid%20overfit).\n",
    "These are the steps for implementing the Random Forest model: \n",
    "- Step 1: A subset of data points and a subset of features is selected for constructing each decision tree. Simply put, n random records and m features are taken from the data set having k number of records.\n",
    "\n",
    "- Step 2: Individual decision trees are constructed for each sample.\n",
    "\n",
    "- Step 3: Each decision tree will generate an output.\n",
    "\n",
    "- Step 4: Final output is considered based on Majority Voting or Averaging for Classification and regression, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f347a-a831-47a4-89a1-a9a6c605c8d7",
   "metadata": {},
   "source": [
    "![rf_algo_viz.jpeg](rf_algo_viz.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05edd82a-19d5-49a5-ba6c-4d0f45929e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=500, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=500, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=500, random_state=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bring in RF model from sklearn and define rf model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "#train the model on our training data\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3528e1fe-05db-4af2-aa8d-749cc28cd70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_score (train):  0.9773763028723524\n",
      "RMSE for training set is 1.4134376081749656\n",
      "R2_score (test):  0.8612575946626428\n",
      "RMSE for test set is 3.233214527492636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predicting R2 Score the Train set results\n",
    "y_pred_rf_train = rf.predict(X_train)\n",
    "r2_score_rf_train = r2_score(y_train, y_pred_rf_train)\n",
    "\n",
    "# Predicting R2 Score the Test set results\n",
    "y_pred_rf_test = rf.predict(X_test)\n",
    "r2_score_rf_test = r2_score(y_test, y_pred_rf_test)\n",
    "\n",
    "# Predicting RMSE the Test set result\n",
    "rmse_rf_train = (np.sqrt(mean_squared_error(y_train, y_pred_rf_train)))\n",
    "rmse_rf_test = (np.sqrt(mean_squared_error(y_test, y_pred_rf_test)))\n",
    "print('R2_score (train): ', r2_score_rf_train)\n",
    "print('RMSE for training set is {}'.format(rmse_rf_train))\n",
    "print('R2_score (test): ', r2_score_rf_test)\n",
    "print(\"RMSE for test set is {}\".format(rmse_rf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adafd39-f8cb-4f19-aaab-f7df49f72f4b",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910eeee-df3e-42e2-b15e-261e18ac11b1",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning method that combines weak models together to create a strong learner that minimizes training errors. This sounds similar to Bagging, and foundationally it is, but there is one key difference. While bagging trains its models in parallel, boosting trains models **Sequentially**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ecb2e9-2d98-46c7-a039-52b83bf48a7d",
   "metadata": {},
   "source": [
    "![boosting_visual.png](boosting_visual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d534c-e926-4713-859c-4f4b8060293b",
   "metadata": {},
   "source": [
    "### 1. Sequential Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80690a-ce57-4baa-b75b-3a2fdba2e948",
   "metadata": {},
   "source": [
    "With Sequential training, a series of models are constructed and with each new model, the weights of the misclassified data in the previous model are changed to get better accuracy. This process helps the algorithm identify the key variables that it needs to focus on to improve its performance. Boosting is usually used for models that have low variance and high bias, which tend to be models experience underfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fb0a6-cc4b-4afe-bd37-06eaee1b4f07",
   "metadata": {},
   "source": [
    "![boosting_visual_process.png](boosting_visual_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35b8cb-1929-43af-a388-a7ad8dd4eb55",
   "metadata": {},
   "source": [
    "### 2. Types of Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08a510-47ab-4cd3-b6c3-ced5d522fe31",
   "metadata": {},
   "source": [
    "Popular types of boosting algorithms include:\n",
    "- **AdaBoost(Adaptive Boosting)**:This method operates iteratively, identifying misclassified data points and adjusting their weights to minimize the training error. The model continues optimize in a sequential fashion until it yields the strongest predictor.  \n",
    "\n",
    "- **Gradient Boosting**:This also works sequentially but unlike AdaBoost, Gradient Boosting trains on the residual errors of the previous predictor. The name, gradient boosting, is used since it combines the [gradient descent](https://www.ibm.com/topics/gradient-descent#:~:text=Gradient%20descent%20is%20an%20optimization,each%20iteration%20of%20parameter%20updates) algorithm and boosting method. \n",
    "\n",
    "- **XGBoost(Extreme Gradient Boosting)**: XGBoost is an implementation of gradient boosting that uses an ensemble of decision trees and gradient boosting to make predictions. XGBoost leverages multiple cores on the CPU, allowing for learning to occur in parallel during training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2352f25-e956-4a91-9213-f2dd55645a5c",
   "metadata": {},
   "source": [
    "### 3. Boosting with the XGBOOST Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fc8ca-a7d6-44ae-b25a-97c05b55033c",
   "metadata": {},
   "source": [
    "Lets test out the XGBoost algorithm on our boston housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383573db-971d-4812-9b05-b8edaff11237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0409c852-9fcd-4091-97b8-aac8dbeb5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6200cf17-ff3a-4d49-8234-e7e64aebf560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror')  #Our XGBoost model\n",
    "xgbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56813c3-5ec5-4076-9a4c-3eb6d8a02600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_score (train):  0.9445743991372995\n",
      "RMSE for training set is 2.20719685494909\n",
      "R2_score (test):  0.9495325463753564\n",
      "RMSE for test set is 1.9391973068772448\n"
     ]
    }
   ],
   "source": [
    "# Predicting R2 Score the Train set results\n",
    "y_pred_xgb_train = rf.predict(X_train)\n",
    "r2_score_xgb_train = r2_score(y_train, y_pred_xgb_train)\n",
    "\n",
    "# Predicting R2 Score the Test set results\n",
    "y_pred_xgb_test = rf.predict(X_test)\n",
    "r2_score_xgb_test = r2_score(y_test, y_pred_xgb_test)\n",
    "\n",
    "# Predicting RMSE the Test set result\n",
    "rmse_xgb_train = (np.sqrt(mean_squared_error(y_train, y_pred_xgb_train)))\n",
    "rmse_xgb_test = (np.sqrt(mean_squared_error(y_test, y_pred_xgb_test)))\n",
    "print('R2_score (train): ', r2_score_xgb_train)\n",
    "print('RMSE for training set is {}'.format(rmse_xgb_train))\n",
    "print('R2_score (test): ', r2_score_xgb_test)\n",
    "print(\"RMSE for test set is {}\".format(rmse_xgb_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03226e4c-bbe7-42e7-8adf-7bb6d0dfc2fa",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f3bb2-d01c-4a3c-aedf-3bf23db5e68b",
   "metadata": {},
   "source": [
    "Boosting and Bagging are the most widely known and used ensemble methods, but another powerful ensemble method used in Machine Learning is **Stacking**. Stacking is a method that uses different types of models to learn some parts of a problem within a dataset. What stacking does is that it builds multiple different learners and use them to build an \"intermediate prediction\", one prediction for each learned model. Afterwards a new model, the final model, is then added to the framework. This model is stacked on top of the others and learns from each of the intermediate predictions to build the final prediction of the framework. This process will most likely give you a better model and might improve overall model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d474ac3-fc0e-46e6-b30b-188e47e6ebd7",
   "metadata": {},
   "source": [
    "![model_stacking.png](model_stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af8e94-3971-431d-ad7e-7e541d73f285",
   "metadata": {},
   "source": [
    "### 1. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334a3ae-05f5-4574-aa3e-80604286cab1",
   "metadata": {},
   "source": [
    "To perform stacking, a method called [**Cross Validation**](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is required. This method is optional for bagging and boosting, but it is absolutely critical for stacking, so lets learn more about it. The gold standard for model evaluation in Machine Learning is train-test-split. You first split your data into two sets, the training set and the test set, train your model on the training set, then test it on the testing set to ensure your model works well on unseen data. This is done to check for overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c85b5-b80c-433e-8ee4-4262b9424e17",
   "metadata": {},
   "source": [
    "Now when doing evaluations on the testing set, there's a chance that your model could mold itself differently after being fit on the testing set, which could result in overfitting on other data brought in after the model is finalized. This is where the *validation* set comes in. With validation your data is split into three sets, the training set, the validation set, and the test set. This can work, but this can reduce the number of samples which can be sued for learning the model and the results are based on picking data points randomly, which could cause more overfitting issues down the line with unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec92da-a7f3-4f6d-bde7-9d3366a973e6",
   "metadata": {},
   "source": [
    "This is where cross validation comes in. When performing cross validation with stacking you first split your data into two sets, the training set and the holdout set. You then *further* split your training data into smaller sets(k sets). Each k subset is then further split *again* where you'll have training sets and a testing set for each k subset. You then bring in the models where one model would be used for each k subset. Once each model has been trained using the k subsets you'd then test them again on the holdout dataset. The average of the predictions for the test set in each k fold would be put into an array and added to the larger training set as a new feature and the same would be done for the holdout set, each getting a new feature added to it. The udpatd datasets would then be used to train the metalearner for the final predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd02172-d67d-4a47-99d2-9a8a1477a908",
   "metadata": {},
   "source": [
    "![stacking_process.png](stacking_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e6f95-f3da-4602-9f89-a51d49b5a599",
   "metadata": {},
   "source": [
    "### 2. Cross Validation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4096583-fc76-427c-a050-88c798675dc1",
   "metadata": {},
   "source": [
    "This is a step by step process of what Cross Validation looks like with stacking: \n",
    "\n",
    "- 1. Gather the models you'd like to use for your data (Random Forest, XGBoost)\n",
    "- 2. Split your data set into a Training and Holdout Dataset \n",
    "- 3. For the training set, split the data into K subsets (k = 5 subsets) and for each subset\n",
    "    - 3.1. For each iteration of K, split the subset into a training set and a test set\n",
    "    - 3.2. Set your current model to train on the training set and then validate it on the test set\n",
    "    - 3.3. After making predictions on the test set, collect all your predictions for each set and add them into a full_y_predict array\n",
    "    - 3.4. Test the model again and make predictions on the Hold out set and call them holdout_pred\n",
    "    - 3.5. Add full_y_predict as a feature on the training set and add holdout_pred as a feature in the hold out set\n",
    "- 4. Return the Training set and the Holdout set with the new features and run them through your metalearner in the next layer to create your final predictions\n",
    "\n",
    "We would predict one fold at a time, with a different model each time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a50f5b-cab0-4411-8d26-54c556f06d1f",
   "metadata": {},
   "source": [
    "![k_fold_crossv.png](k_fold_crossv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78e06c-978f-4704-925c-c513037eabb4",
   "metadata": {},
   "source": [
    "### 3. Implementing Stacking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9c540-0fbe-4cba-8126-0a7f8e4295ed",
   "metadata": {},
   "source": [
    "Lets actually implement model stacking using the 4 models we've used so far on our data set, Linear Regression, Random Forest, and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45dded37-f7c9-4e1a-ab41-a878aa432cfc",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /opt/anaconda3/lib/python3.9/site-packages (0.22.0)\r\n",
      "Requirement already satisfied: scipy>=1.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from mlxtend) (1.10.1)\r\n",
      "Requirement already satisfied: pandas>=0.24.2 in /opt/anaconda3/lib/python3.9/site-packages (from mlxtend) (2.0.0)\r\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from mlxtend) (3.7.1)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from mlxtend) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.13.2 in /opt/anaconda3/lib/python3.9/site-packages (from mlxtend) (1.2.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from mlxtend) (65.6.3)\r\n",
      "Requirement already satisfied: numpy>=1.16.2 in /opt/anaconda3/lib/python3.9/site-packages (from mlxtend) (1.24.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (4.39.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (5.12.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (1.0.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (9.4.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (23.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.24.2->mlxtend) (2023.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.24.2->mlxtend) (2022.7)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=1.0.2->mlxtend) (3.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.0->mlxtend) (3.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a2902-70f2-4ae0-bab5-0362d44d5ea3",
   "metadata": {},
   "source": [
    "We'll define our training sets and our testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab07cd01-076d-494e-ab02-ba7059b28e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (354, 13)\n",
      "Shape of X_test:  (152, 13)\n",
      "Shape of y_train:  (354,)\n",
      "Shape of y_test (152,)\n"
     ]
    }
   ],
   "source": [
    "X = bh.drop('MEDV', axis =1)\n",
    "y = bh['MEDV']\n",
    "# Lets split our training set and our testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 25)\n",
    "#Now take a look at the distributions \n",
    "print(\"Shape of X_train: \",X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d587658-392c-42cf-8d67-e6fc0fc4dc9c",
   "metadata": {},
   "source": [
    "Now lets choose and define the models we need for our learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9a468d3-8083-4e61-8ea5-1e6c08897a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "l_model = LinearRegression()\n",
    "rf = RandomForestRegressor()\n",
    "xgb = XGBRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2e63a-fc72-4a9c-9400-b1dedc539a43",
   "metadata": {},
   "source": [
    "Lets bring in our performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56126099-d6fc-4b0a-9d33-b8b37b4b36c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30db28-42c0-4bae-ac7d-85683aa53547",
   "metadata": {},
   "source": [
    "And then lets now build our learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d7a49b1-fa33-443a-b629-eac20bf88c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE For Stacking is 2.8714648435255357\n",
      "R2 score for Stacking is 0.8905672826864751\n"
     ]
    }
   ],
   "source": [
    "stack = StackingCVRegressor(regressors=(l_model,rf, xgb),\n",
    "                            meta_regressor=xgb, cv=12,\n",
    "                            use_features_in_secondary=True,\n",
    "                            store_train_meta_features=True,\n",
    "                            shuffle=False,\n",
    "                            random_state=42)\n",
    "\n",
    "stack.fit(X_train.values, y_train.values)\n",
    "pred = stack.predict(X_test.values)\n",
    "rmse_stack = (np.sqrt(mean_squared_error(y_test, pred)))\n",
    "score = r2_score(y_test, pred)\n",
    "print('RMSE For Stacking is {}'.format(rmse_stack))\n",
    "print('R2 score for Stacking is {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425ae87-053d-40a3-8e2a-4d4de513554d",
   "metadata": {},
   "source": [
    "This learner has a RMSE of about 3 and a 90% accuracy score which tells us that it is able to explain 90% of the variation in our model with the test set. These are great metrics and shows how powerful our stacking model can be for gaining better insights into our data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}